# version: '3.8'

services:
  zookeeper:
    image: zookeeper:latest
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888
    restart: on-failure

  spark-master:
    build:
      context: ../docker/master
      dockerfile: Dockerfile
    image: spark-master:latest
    hostname: spark-master
    container_name: spark-master
    env_file:
      - ../config/.env
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_DRIVER_HOST=spark-master
      - SPARK_HOME=/opt/spark
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - SPARK_LOG_DIR=/opt/spark/logs
      - SPARK_NETWORK_MAXREMOTEFRAMESIZE=2047
      - SPARK_RPC_MESSAGE_MAXSIZE=2047
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/tmp/spark-events
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-network
    volumes:
      - ../data:/opt/spark-data
      - ../benchmarks:/opt/spark/benchmarks
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ../logs:/opt/spark/logs
      - /tmp/spark-events:/tmp/spark-events

  spark-worker:
    build:
      context: ../docker/worker
      dockerfile: Dockerfile
    image: spark-worker:latest
    depends_on:
      - spark-master
    env_file:
      - ../config/.env
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HOME=/opt/spark
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 10
    restart: on-failure
    networks:
      - spark-network
    volumes:
      - ../data:/opt/spark-data
      - ../logs:/opt/spark/logs
      - /tmp/spark-events:/tmp/spark-events

  spark-submit:
    image: spark-master:latest
    container_name: spark-submit
    depends_on:
      - spark-master
      - spark-worker
    env_file:
      - ../config/.env
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
    networks:
      - spark-network
    volumes:
      - ../benchmarks:/opt/spark/benchmarks
      - ../data:/opt/spark-data
      - /tmp/spark-events:/tmp/spark-events
    entrypoint: ["/bin/sh", "-c", "sleep 10 && /opt/spark/bin/spark-submit --master spark://spark-master:7077 --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=/tmp/spark-events /opt/spark/benchmarks/run_benchmark_cloud.py"]
    restart: "no"
    
  scale-monitor:
    build:
      context: ../docker/monitor
      dockerfile: Dockerfile
    image: scale-monitor:latest
    container_name: scale-monitor
    depends_on:
      - spark-master
    env_file:
      - ../config/.env
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ../docker/monitor/scripts:/config/scripts
      - ../config:/config
    networks:
      - spark-network
    restart: no

networks:
  spark-network:
    driver: bridge
    external: true