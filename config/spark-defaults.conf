# Specify the Spark master URL for workers to connect to
spark.master                     spark://spark-master:7077

# Configure maximum RPC message size
spark.rpc.message.maxSize        2047m

# Configure max result size for Spark driver
spark.driver.maxResultSize       1g

# Configure executor memory and cores
spark.executor.memory            1g
spark.executor.cores             1

# Set memory overhead for executors (helps with containerized environments)
spark.executor.memoryOverhead    512m

# Network settings for large frames and high-throughput communication
spark.network.timeout            120s
spark.network.maxRemoteBlockSizeFetchToMem      2047m
spark.network.maxRemoteBlockSizeFetchToDisk     1g
spark.executor.heartbeatInterval 60s

# Shuffle configurations for better fault tolerance and performance with Zookeeper
spark.shuffle.service.enabled                       true
spark.shuffle.io.maxRetries                        10
spark.shuffle.io.retryWait                         15s
spark.shuffle.service.index.cache.size             2048
spark.shuffle.service.index.cache.executorExpireThreshold  60m

# Zookeeper configuration for dynamic resource allocation and shuffle service
spark.dynamicAllocation.enabled                    true
spark.dynamicAllocation.minExecutors               1
spark.dynamicAllocation.initialExecutors           2
spark.dynamicAllocation.maxExecutors               10
spark.dynamicAllocation.shuffleTracking.enabled    true
spark.dynamicAllocation.zookeeper.url              zookeeper:2181  # Ensure this matches your Docker service name for Zookeeper

# Zookeeper directories for fault tolerance
spark.shuffle.service.zookeeperDir                 /spark-shuffle
spark.dynamicAllocation.zookeeper.dir              /spark-dynamic-allocation

# Spark event log settings for tracking and monitoring
spark.eventLog.enabled           true
spark.eventLog.dir               /opt/spark/logs/events
spark.history.fs.logDirectory    /opt/spark/logs/history

# Set the driver bind address (helpful if the driver runs inside a container)
spark.driver.host                spark-master

# AWS S3 Configuration for accessing data
spark.hadoop.fs.s3a.access.key ${AWS_ACCESS_KEY_ID}
spark.hadoop.fs.s3a.secret.key ${AWS_SECRET_ACCESS_KEY}
spark.hadoop.fs.s3a.endpoint s3.amazonaws.com

# Additional S3 configuration for optimized access
spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access       true
spark.hadoop.fs.s3a.connection.maximum      100
spark.hadoop.fs.s3a.fast.upload             true
spark.hadoop.fs.s3a.buffer.dir              /tmp/spark-s3a-buffer  # Set local temp directory for S3 buffers
spark.hadoop.fs.s3a.threads.core            10  # Adjust number of threads for S3 upload/downloads
spark.hadoop.fs.s3a.fast.upload.buffer      bytebuffer  # Use bytebuffer for faster uploads